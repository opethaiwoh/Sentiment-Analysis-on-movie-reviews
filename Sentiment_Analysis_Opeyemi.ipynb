{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5d2f786a",
      "metadata": {
        "id": "5d2f786a"
      },
      "source": [
        "## Performing sentimental Analysis on amazon and yelp dataset reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7b7f0e8",
      "metadata": {
        "id": "c7b7f0e8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "import os\n",
        "# print(os.listdir(\"../input\"))\n",
        "plt.style.use('ggplot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e310c34",
      "metadata": {
        "id": "2e310c34"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "df = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19f5fb2e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19f5fb2e",
        "outputId": "ba56942d-ff68-454f-d8cd-6e38e8c925c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                sentence label  source\n",
            "0      So there is no way for me to plug it in here i...     0  amazon\n",
            "1                            Good case, Excellent value.     1  amazon\n",
            "2                                 Great for the jawbone.     1  amazon\n",
            "3      Tied to charger for conversations lasting more...     0  amazon\n",
            "4                                      The mic is great.     1  amazon\n",
            "...                                                  ...   ...     ...\n",
            "51996  \"Towards the end of the movie, I felt it was t...   NaN    imdb\n",
            "51997  \"This is the kind of movie that my enemies con...   NaN    imdb\n",
            "51998  \"I saw 'Descent' last night at the Stockholm F...   NaN    imdb\n",
            "51999  \"Some films that you pick up for a pound turn ...   NaN    imdb\n",
            "52000  \"This is one of the dumbest films, I've ever s...   NaN    imdb\n",
            "\n",
            "[52001 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "dl_yelp = pd.read_csv('yelp_data.txt', sep='\\t', names=['sentence', 'label'], encoding = 'utf-8', header=None)\n",
        "dl_yelp = pd.DataFrame(dl_yelp)\n",
        "dl_yelp['source'] = 'yelp'\n",
        "dl_yelp.values\n",
        "dl_yelp\n",
        "\n",
        "\n",
        "#amazon\n",
        "dm_amazon = pd.read_csv('amazon_data.txt', sep='\\t', names=['sentence', 'label'], encoding = 'utf-8')\n",
        "dm_amazon = pd.DataFrame(dm_amazon)\n",
        "dm_amazon['source'] = 'amazon'\n",
        "dm_amazon.values\n",
        "dm_amazon\n",
        "\n",
        "#imdb\n",
        "dv_movie = pd.read_csv('movie_data.csv', quoting=csv.QUOTE_NONE, names=['sentence', 'label'], sep=\"\\t\", encoding = \"utf-8\")\n",
        "dv_movie = dv_movie.rename(columns={'review':'sentence'})\n",
        "dv_movie = dv_movie.rename(columns={'sentiment':'label'})\n",
        "dv_movie['source'] = 'imdb'\n",
        "dv_movie.values\n",
        "dv_movie\n",
        "\n",
        "df = df.append(dm_amazon, ignore_index=True)\n",
        "df = df.append(dl_yelp, ignore_index=True)\n",
        "df = df.append(dv_movie, ignore_index=True)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be4afd80",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be4afd80",
        "outputId": "d21fe875-fa5d-4cb3-d5d2-1158bb721263"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[                                              sentence  label source\n",
            "0    So there is no way for me to plug it in here i...      0   yelp\n",
            "1                          Good case, Excellent value.      1   yelp\n",
            "2                               Great for the jawbone.      1   yelp\n",
            "3    Tied to charger for conversations lasting more...      0   yelp\n",
            "4                                    The mic is great.      1   yelp\n",
            "..                                                 ...    ...    ...\n",
            "995  The screen does get smudged easily because it ...      0   yelp\n",
            "996  What a piece of junk.. I lose more calls on th...      0   yelp\n",
            "997                       Item Does Not Match Picture.      0   yelp\n",
            "998  The only thing that disappoint me is the infra...      0   yelp\n",
            "999  You can not answer calls with the unit, never ...      0   yelp\n",
            "\n",
            "[1000 rows x 3 columns]]\n",
            "[                                              sentence  label source\n",
            "0    So there is no way for me to plug it in here i...      0   yelp\n",
            "1                          Good case, Excellent value.      1   yelp\n",
            "2                               Great for the jawbone.      1   yelp\n",
            "3    Tied to charger for conversations lasting more...      0   yelp\n",
            "4                                    The mic is great.      1   yelp\n",
            "..                                                 ...    ...    ...\n",
            "995  The screen does get smudged easily because it ...      0   yelp\n",
            "996  What a piece of junk.. I lose more calls on th...      0   yelp\n",
            "997                       Item Does Not Match Picture.      0   yelp\n",
            "998  The only thing that disappoint me is the infra...      0   yelp\n",
            "999  You can not answer calls with the unit, never ...      0   yelp\n",
            "\n",
            "[1000 rows x 3 columns],                                               sentence  label  source\n",
            "0    So there is no way for me to plug it in here i...      0  amazon\n",
            "1                          Good case, Excellent value.      1  amazon\n",
            "2                               Great for the jawbone.      1  amazon\n",
            "3    Tied to charger for conversations lasting more...      0  amazon\n",
            "4                                    The mic is great.      1  amazon\n",
            "..                                                 ...    ...     ...\n",
            "995  The screen does get smudged easily because it ...      0  amazon\n",
            "996  What a piece of junk.. I lose more calls on th...      0  amazon\n",
            "997                       Item Does Not Match Picture.      0  amazon\n",
            "998  The only thing that disappoint me is the infra...      0  amazon\n",
            "999  You can not answer calls with the unit, never ...      0  amazon\n",
            "\n",
            "[1000 rows x 3 columns]]\n",
            "[                                              sentence  label source\n",
            "0    So there is no way for me to plug it in here i...      0   yelp\n",
            "1                          Good case, Excellent value.      1   yelp\n",
            "2                               Great for the jawbone.      1   yelp\n",
            "3    Tied to charger for conversations lasting more...      0   yelp\n",
            "4                                    The mic is great.      1   yelp\n",
            "..                                                 ...    ...    ...\n",
            "995  The screen does get smudged easily because it ...      0   yelp\n",
            "996  What a piece of junk.. I lose more calls on th...      0   yelp\n",
            "997                       Item Does Not Match Picture.      0   yelp\n",
            "998  The only thing that disappoint me is the infra...      0   yelp\n",
            "999  You can not answer calls with the unit, never ...      0   yelp\n",
            "\n",
            "[1000 rows x 3 columns],                                               sentence  label  source\n",
            "0    So there is no way for me to plug it in here i...      0  amazon\n",
            "1                          Good case, Excellent value.      1  amazon\n",
            "2                               Great for the jawbone.      1  amazon\n",
            "3    Tied to charger for conversations lasting more...      0  amazon\n",
            "4                                    The mic is great.      1  amazon\n",
            "..                                                 ...    ...     ...\n",
            "995  The screen does get smudged easily because it ...      0  amazon\n",
            "996  What a piece of junk.. I lose more calls on th...      0  amazon\n",
            "997                       Item Does Not Match Picture.      0  amazon\n",
            "998  The only thing that disappoint me is the infra...      0  amazon\n",
            "999  You can not answer calls with the unit, never ...      0  amazon\n",
            "\n",
            "[1000 rows x 3 columns],                                                 sentence label source\n",
            "0                                       review,sentiment   NaN   imdb\n",
            "1      \"I went and saw this movie last night after be...   NaN   imdb\n",
            "2      \"Actor turned director Bill Paxton follows up ...   NaN   imdb\n",
            "3      \"As a recreational golfer with some knowledge ...   NaN   imdb\n",
            "4      \"I saw this film in a sneak preview, and it is...   NaN   imdb\n",
            "...                                                  ...   ...    ...\n",
            "49996  \"Towards the end of the movie, I felt it was t...   NaN   imdb\n",
            "49997  \"This is the kind of movie that my enemies con...   NaN   imdb\n",
            "49998  \"I saw 'Descent' last night at the Stockholm F...   NaN   imdb\n",
            "49999  \"Some films that you pick up for a pound turn ...   NaN   imdb\n",
            "50000  \"This is one of the dumbest films, I've ever s...   NaN   imdb\n",
            "\n",
            "[50001 rows x 3 columns]]\n",
            "                                            sentence label source\n",
            "0  So there is no way for me to plug it in here i...     0   yelp\n",
            "1                        Good case, Excellent value.     1   yelp\n",
            "2                             Great for the jawbone.     1   yelp\n",
            "3  Tied to charger for conversations lasting more...     0   yelp\n",
            "4                                  The mic is great.     1   yelp\n",
            "                                                sentence label source\n",
            "49996  \"Towards the end of the movie, I felt it was t...   NaN   imdb\n",
            "49997  \"This is the kind of movie that my enemies con...   NaN   imdb\n",
            "49998  \"I saw 'Descent' last night at the Stockholm F...   NaN   imdb\n",
            "49999  \"Some films that you pick up for a pound turn ...   NaN   imdb\n",
            "50000  \"This is one of the dumbest films, I've ever s...   NaN   imdb\n"
          ]
        }
      ],
      "source": [
        "filepath_dict = {\n",
        "                 'yelp':   '/content/amazon_data.txt',\n",
        "                 'amazon': '/content/amazon_data.txt',\n",
        "                 'imdb': '/content/movie_data.csv'\n",
        "}\n",
        "                                  \n",
        "\n",
        "df_list = []\n",
        "for source, filepath in filepath_dict.items():\n",
        "    df = pd.read_csv(filepath, names=['sentence', 'label'], quoting=csv.QUOTE_NONE, delimiter='\\t', encoding = 'utf-8')\n",
        "    df['source'] = source  # Add another column filled with the source name\n",
        "    #df['label'] = pd.to_numeric(df['label'], downcast=\"float\")\n",
        "    df_list.append(df)\n",
        "    print(df_list)\n",
        "\n",
        "df = pd.concat(df_list)\n",
        "#df['label'] = df['label'].replace(np.nan, 0)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "#print(df.iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40b29b5b",
      "metadata": {
        "id": "40b29b5b"
      },
      "outputs": [],
      "source": [
        "# Using both CountVectorizer and TfidfVectorizer separately of the sklearn library\n",
        "#perform the Logistic regression classification in the IMDb dataset and evaluate the accuracies in the test set.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_yelp = df[df['source'] == 'yelp']\n",
        "df_amazon = df[df['source'] == 'amazon']\n",
        "df_imdb = df[df['source'] == 'imdb']\n",
        "\n",
        "sentences = df_yelp['sentence'].values\n",
        "y = df_yelp['label'].values\n",
        "\n",
        "sentences1 = df_amazon['sentence'].values\n",
        "y1 = df_amazon['label'].values\n",
        "\n",
        "sentences2 = df_imdb['sentence'].values\n",
        "y2 = df_imdb['label'].values\n",
        "\n",
        "# YELP\n",
        "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.30, random_state=1000)\n",
        "\n",
        "# AMAZON\n",
        "sentences1_train, sentences1_test, y1_train, y1_test = train_test_split(sentences1, y1, \n",
        "                                                                        test_size=0.30, random_state=1000)\n",
        "# IDMB\n",
        "sentences2_train, sentences2_test, y2_train, y2_test = train_test_split(sentences2, y2, \n",
        "                                                                        test_size=0.30, random_state=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "592b312c",
      "metadata": {
        "id": "592b312c"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "import string \n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support as score, f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d3dd8f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d3dd8f9",
        "outputId": "0acc0bf8-9853-427d-c132-cdde78c1fbb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "                                              review  sentiment\n",
            "0  I went and saw this movie last night after bei...          1\n",
            "1  Actor turned director Bill Paxton follows up h...          1\n",
            "2  As a recreational golfer with some knowledge o...          1\n",
            "3  I saw this film in a sneak preview, and it is ...          1\n",
            "4  Bill Paxton has taken the true story of the 19...          1\n",
            "(50000, 2)\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "stopwords.words('english')\n",
        "print(stopwords.words('english'))\n",
        "m_data = pd.read_csv(\"/content/movie_data.csv\")\n",
        "print(m_data.head())\n",
        "print(m_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "189aaa12",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "189aaa12",
        "outputId": "c0018f5f-c480-4052-d8d0-e246588615b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  sentiment  \\\n",
              "0  I went and saw this movie last night after bei...          1   \n",
              "1  Actor turned director Bill Paxton follows up h...          1   \n",
              "2  As a recreational golfer with some knowledge o...          1   \n",
              "3  I saw this film in a sneak preview, and it is ...          1   \n",
              "4  Bill Paxton has taken the true story of the 19...          1   \n",
              "\n",
              "                                             no_punc  \\\n",
              "0  i went and saw this movie last night after bei...   \n",
              "1  actor turned director bill paxton follows up h...   \n",
              "2  as a recreational golfer with some knowledge o...   \n",
              "3  i saw this film in a sneak preview and it is d...   \n",
              "4  bill paxton has taken the true story of the 19...   \n",
              "\n",
              "                                      tokenized_Data  \\\n",
              "0  [i, went, and, saw, this, movie, last, night, ...   \n",
              "1  [actor, turned, director, bill, paxton, follow...   \n",
              "2  [as, a, recreational, golfer, with, some, know...   \n",
              "3  [i, saw, this, film, in, a, sneak, preview, an...   \n",
              "4  [bill, paxton, has, taken, the, true, story, o...   \n",
              "\n",
              "                                             no_stop  \\\n",
              "0  [went, saw, movie, last, night, coaxed, friend...   \n",
              "1  [actor, turned, director, bill, paxton, follow...   \n",
              "2  [recreational, golfer, knowledge, sports, hist...   \n",
              "3  [saw, film, sneak, preview, delightful, cinema...   \n",
              "4  [bill, paxton, taken, true, story, 1913, us, g...   \n",
              "\n",
              "                                          lemmatized  \n",
              "0  went saw movie last night coaxed friend mine i...  \n",
              "1  actor turned director bill paxton follows prom...  \n",
              "2  recreational golfer knowledge sport history pl...  \n",
              "3  saw film sneak preview delightful cinematograp...  \n",
              "4  bill paxton taken true story 1913 u golf open ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9ee7d8d5-c5e9-4ede-9432-87ffd57313a2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>no_punc</th>\n",
              "      <th>tokenized_Data</th>\n",
              "      <th>no_stop</th>\n",
              "      <th>lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I went and saw this movie last night after bei...</td>\n",
              "      <td>1</td>\n",
              "      <td>i went and saw this movie last night after bei...</td>\n",
              "      <td>[i, went, and, saw, this, movie, last, night, ...</td>\n",
              "      <td>[went, saw, movie, last, night, coaxed, friend...</td>\n",
              "      <td>went saw movie last night coaxed friend mine i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Actor turned director Bill Paxton follows up h...</td>\n",
              "      <td>1</td>\n",
              "      <td>actor turned director bill paxton follows up h...</td>\n",
              "      <td>[actor, turned, director, bill, paxton, follow...</td>\n",
              "      <td>[actor, turned, director, bill, paxton, follow...</td>\n",
              "      <td>actor turned director bill paxton follows prom...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>As a recreational golfer with some knowledge o...</td>\n",
              "      <td>1</td>\n",
              "      <td>as a recreational golfer with some knowledge o...</td>\n",
              "      <td>[as, a, recreational, golfer, with, some, know...</td>\n",
              "      <td>[recreational, golfer, knowledge, sports, hist...</td>\n",
              "      <td>recreational golfer knowledge sport history pl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I saw this film in a sneak preview, and it is ...</td>\n",
              "      <td>1</td>\n",
              "      <td>i saw this film in a sneak preview and it is d...</td>\n",
              "      <td>[i, saw, this, film, in, a, sneak, preview, an...</td>\n",
              "      <td>[saw, film, sneak, preview, delightful, cinema...</td>\n",
              "      <td>saw film sneak preview delightful cinematograp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bill Paxton has taken the true story of the 19...</td>\n",
              "      <td>1</td>\n",
              "      <td>bill paxton has taken the true story of the 19...</td>\n",
              "      <td>[bill, paxton, has, taken, the, true, story, o...</td>\n",
              "      <td>[bill, paxton, taken, true, story, 1913, us, g...</td>\n",
              "      <td>bill paxton taken true story 1913 u golf open ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ee7d8d5-c5e9-4ede-9432-87ffd57313a2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9ee7d8d5-c5e9-4ede-9432-87ffd57313a2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9ee7d8d5-c5e9-4ede-9432-87ffd57313a2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#Removing punctuations from entire dataset\n",
        "ps = string.punctuation\n",
        "ps\n",
        "\n",
        "#Function for removing punctions\n",
        "def remove_punc(text):\n",
        "    clean = \"\".join([x.lower() for x in text if x not in ps])\n",
        "    return clean\n",
        "\n",
        "#Applying the 'remove_punc' function to entire dataset\n",
        "m_data['no_punc'] = m_data['review'].apply(lambda z:remove_punc(z))\n",
        "\n",
        "#Function for Tokenizing entire data for representing every word as datapoint\n",
        "def tokenize(text):\n",
        "    tokens = re.split(\"\\W+\",text)\n",
        "    return tokens\n",
        "\n",
        "#Applying the 'tokenize' function to entire dataset\n",
        "m_data['tokenized_Data'] = m_data['no_punc'].apply(lambda z:tokenize(z))\n",
        "\n",
        "#Importing stopwords from NLTK Library to remove stopwords now that we have tokenized it\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "#Function for removing stopwords from single row\n",
        "def remove_stopwords(tokenized_words):\n",
        "    Ligit_text=[word for word in tokenized_words if word not in stopwords]\n",
        "    return Ligit_text\n",
        "\n",
        "#Applying the function 'remove_stopwords' from the entire dataset\n",
        "m_data[\"no_stop\"] = m_data[\"tokenized_Data\"].apply(lambda z:remove_stopwords(z))\n",
        "\n",
        "#Importing 'WordNetLemmatizer' as lemmatizing function to find lemma's of words\n",
        "wnl = nltk.wordnet.WordNetLemmatizer()\n",
        "\n",
        "#Function for lemmatizing the tokenzied text\n",
        "def lemmatizing(tokenized_text):\n",
        "    lemma = [wnl.lemmatize(word) for word in tokenized_text]\n",
        "    return lemma\n",
        "\n",
        "#Applying the 'lemmatizing' function to entire dataset     \n",
        "m_data['lemmatized'] = m_data['no_stop'].apply(lambda z:lemmatizing(z))\n",
        "\n",
        "m_data['lemmatized'] = [\" \".join(review) for review in m_data['lemmatized'].values]\n",
        "\n",
        "m_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40033473",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40033473",
        "outputId": "f07fe31e-e2eb-4766-ac04-bd6403cb9b98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000,)\n",
            "(25000,)\n",
            "(25000,)\n",
            "(25000,)\n"
          ]
        }
      ],
      "source": [
        "#Splitting data into smaller dataframes for the purpose of Training and Testing\n",
        "x1 = m_data.iloc[:25000,5]\n",
        "x2 = m_data.iloc[25000:50000,5]\n",
        "y1 = m_data.iloc[:25000,1]\n",
        "y2 = m_data.iloc[25000:50000,1]\n",
        "print(x1.shape)\n",
        "print(x2.shape)\n",
        "print(y1.shape)\n",
        "print(y2.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d56740c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d56740c",
        "outputId": "2a82a191-c0e4-4c49-dd9b-d13d6d26dd8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 109532)\n",
            "(25000, 109532)\n"
          ]
        }
      ],
      "source": [
        "count_vect = CountVectorizer()\n",
        "xv_train = count_vect.fit_transform(x1.values)\n",
        "xv_test = count_vect.transform(x2.values)\n",
        "print(xv_train.shape)\n",
        "print(xv_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13c86be3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13c86be3",
        "outputId": "343fc39e-3eb4-40cc-a81c-4bfdcc8c9836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 1811184)\n",
            "(25000, 1811184)\n"
          ]
        }
      ],
      "source": [
        "#Declaring and applying TFIDF functions to train and test data\n",
        "tdf_vect = TfidfVectorizer(ngram_range=(1,2))\n",
        "tdf_train = tdf_vect.fit_transform(x1.values)\n",
        "tdf_test=tdf_vect.transform(x2.values)\n",
        "print(tdf_train.shape)\n",
        "print(tdf_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5873577c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5873577c",
        "outputId": "1bfcd013-dbcd-4c55-a0de-c7a20d300e5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score 89.14507566753824\n",
            "Confusion Matrix [[11009  1466]\n",
            " [ 1274 11251]] \n",
            "\n",
            "Classification               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.88      0.89     12475\n",
            "           1       0.88      0.90      0.89     12525\n",
            "\n",
            "    accuracy                           0.89     25000\n",
            "   macro avg       0.89      0.89      0.89     25000\n",
            "weighted avg       0.89      0.89      0.89     25000\n",
            " \n",
            "\n",
            "Accuracy Score 89.03999999999999\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "tdf_vect = TfidfVectorizer()\n",
        "x_tdf = tdf_vect.fit_transform(m_data[\"lemmatized\"])\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_tdf,m_data[\"sentiment\"],test_size=0.5)\n",
        "\n",
        "lg = LogisticRegression(penalty='l2',random_state=0, solver='lbfgs', multi_class='auto', max_iter=500)\n",
        "lg.fit(x_train,y_train)\n",
        "y_prediction = lg.predict(x_test)\n",
        "f1 = f1_score(y_prediction,y_test)\n",
        "print('F1 score',f1*100)\n",
        "print('Confusion Matrix', confusion_matrix(y_test,y_prediction), \"\\n\")\n",
        "print('Classification', classification_report(y_test,y_prediction), \"\\n\")\n",
        "print('Accuracy Score', accuracy_score(y_test, y_prediction)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3380f3c3",
      "metadata": {
        "id": "3380f3c3"
      },
      "outputs": [],
      "source": [
        "#Classifying the Amazon dataset using Logistic Regression and Neural Network\n",
        "#compare the performances and show the confusion matrices.\n",
        "dm_amazon = pd.read_csv('/content/amazon_data.txt', sep='\\t', names=['sentence', 'label'], encoding = 'unicode_escape')\n",
        "dm_amazon = pd.DataFrame(dm_amazon)\n",
        "dm_amazon['source'] = 'amazon'\n",
        "dm_amazon.values\n",
        "dm_amazon\n",
        "ax = dm_amazon['sentence'].values\n",
        "ay = dm_amazon['label'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dfa5ad0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dfa5ad0",
        "outputId": "24602619-16d1-416e-df94-51ec5e93d066"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 140)\t1\n",
            "  (0, 183)\t1\n",
            "  (0, 602)\t1\n",
            "  (0, 656)\t1\n",
            "  (0, 686)\t1\n",
            "  (0, 883)\t1\n",
            "  (0, 941)\t1\n",
            "  (0, 1138)\t1\n",
            "  (0, 1272)\t1\n",
            "  (0, 1285)\t1\n",
            "  (0, 1340)\t1\n",
            "  (0, 1369)\t1\n",
            "  (0, 1474)\t1\n",
            "  (1, 56)\t1\n",
            "  (1, 74)\t1\n",
            "  (1, 100)\t1\n",
            "  (1, 163)\t1\n",
            "  (1, 389)\t1\n",
            "  (1, 524)\t1\n",
            "  (1, 688)\t1\n",
            "  (1, 691)\t1\n",
            "  (1, 717)\t1\n",
            "  (1, 845)\t1\n",
            "  (1, 866)\t1\n",
            "  (1, 1458)\t1\n",
            "  :\t:\n",
            "  (697, 939)\t1\n",
            "  (697, 1023)\t1\n",
            "  (697, 1109)\t1\n",
            "  (697, 1132)\t1\n",
            "  (697, 1272)\t1\n",
            "  (697, 1333)\t1\n",
            "  (697, 1423)\t1\n",
            "  (697, 1428)\t1\n",
            "  (698, 70)\t1\n",
            "  (698, 73)\t1\n",
            "  (698, 463)\t1\n",
            "  (698, 524)\t1\n",
            "  (698, 618)\t1\n",
            "  (698, 686)\t1\n",
            "  (698, 688)\t1\n",
            "  (698, 881)\t1\n",
            "  (698, 985)\t1\n",
            "  (698, 994)\t1\n",
            "  (698, 1054)\t1\n",
            "  (698, 1272)\t1\n",
            "  (698, 1428)\t1\n",
            "  (698, 1468)\t1\n",
            "  (699, 109)\t1\n",
            "  (699, 1218)\t1\n",
            "  (699, 1401)\t1\n",
            "  (0, 129)\t1\n",
            "  (0, 1288)\t1\n",
            "  (1, 140)\t1\n",
            "  (1, 414)\t1\n",
            "  (1, 692)\t1\n",
            "  (1, 1272)\t1\n",
            "  (2, 74)\t1\n",
            "  (2, 509)\t1\n",
            "  (2, 580)\t1\n",
            "  (2, 742)\t1\n",
            "  (2, 1164)\t1\n",
            "  (3, 74)\t1\n",
            "  (3, 508)\t1\n",
            "  (3, 688)\t1\n",
            "  (3, 994)\t1\n",
            "  (3, 1285)\t1\n",
            "  (3, 1403)\t1\n",
            "  (3, 1449)\t1\n",
            "  (4, 26)\t1\n",
            "  (4, 459)\t1\n",
            "  (4, 508)\t1\n",
            "  (4, 686)\t1\n",
            "  (4, 994)\t1\n",
            "  (4, 1285)\t1\n",
            "  (4, 1471)\t1\n",
            "  :\t:\n",
            "  (298, 574)\t1\n",
            "  (298, 691)\t1\n",
            "  (298, 986)\t1\n",
            "  (298, 1041)\t1\n",
            "  (298, 1164)\t1\n",
            "  (298, 1270)\t1\n",
            "  (299, 51)\t1\n",
            "  (299, 56)\t1\n",
            "  (299, 126)\t1\n",
            "  (299, 183)\t1\n",
            "  (299, 216)\t1\n",
            "  (299, 486)\t1\n",
            "  (299, 500)\t1\n",
            "  (299, 524)\t1\n",
            "  (299, 656)\t1\n",
            "  (299, 758)\t1\n",
            "  (299, 827)\t1\n",
            "  (299, 883)\t1\n",
            "  (299, 898)\t1\n",
            "  (299, 1124)\t1\n",
            "  (299, 1272)\t2\n",
            "  (299, 1285)\t1\n",
            "  (299, 1304)\t1\n",
            "  (299, 1331)\t1\n",
            "  (299, 1380)\t1\n"
          ]
        }
      ],
      "source": [
        "dt_train, dt_test, y_train, y_test = train_test_split(ax, ay, test_size=0.30, random_state=1000)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(dt_train)\n",
        "dt_train_vect=vectorizer.transform(dt_train)\n",
        "dt_test_vect=vectorizer.transform(dt_test)\n",
        "print(dt_train_vect)\n",
        "print(dt_test_vect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c792437",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c792437",
        "outputId": "88842ace-c2e5-4a2b-f150-93bfa959efc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibLinear]"
          ]
        }
      ],
      "source": [
        "#Using logistic regression\n",
        "slog_reg = LogisticRegression(verbose=1, solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000)\n",
        "model=slog_reg.fit(dt_train_vect, y_train)\n",
        "lscore = slog_reg.score(dt_test_vect, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bf550ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bf550ea",
        "outputId": "dda46011-224f-4b01-a995-6cd9ed607f81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.79\n"
          ]
        }
      ],
      "source": [
        "#Using neural network\n",
        "print(\"Accuracy:\", lscore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b074e079",
      "metadata": {
        "id": "b074e079"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6514b3ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6514b3ec",
        "outputId": "8d20a7b0-9c05-4f86-edcd-d5a20aac0db2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_4 (Dense)             (None, 10)                14810     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,821\n",
            "Trainable params: 14,821\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "input_dim = dt_train_vect.shape[1]  # Number of features\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='adam', \n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a045e2e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a045e2e2",
        "outputId": "0c68b009-88fe-4df0-ef28-ae0f1d6f611a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential_2/dense_4/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential_2/dense_4/embedding_lookup_sparse/Reshape:0\", shape=(None, 10), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential_2/dense_4/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70/70 [==============================] - 1s 2ms/step - loss: 0.6847 - accuracy: 0.5886\n",
            "Epoch 2/25\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.6221 - accuracy: 0.8471\n",
            "Epoch 3/25\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.5313 - accuracy: 0.9086\n",
            "Epoch 4/25\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.4264 - accuracy: 0.9400\n",
            "Epoch 5/25\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.3349 - accuracy: 0.9600\n",
            "Epoch 6/25\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.2642 - accuracy: 0.9743\n",
            "Epoch 7/25\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.2093 - accuracy: 0.9829\n",
            "Epoch 8/25\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1682 - accuracy: 0.9929\n",
            "Epoch 9/25\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1373 - accuracy: 0.9943\n",
            "Epoch 10/25\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.1133 - accuracy: 0.9957\n",
            "Epoch 11/25\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0943 - accuracy: 0.9957\n",
            "Epoch 12/25\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0795 - accuracy: 0.9957\n",
            "Epoch 13/25\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0674 - accuracy: 0.9971\n",
            "Epoch 14/25\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0576 - accuracy: 0.9971\n",
            "Epoch 15/25\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0498 - accuracy: 0.9971\n",
            "Epoch 16/25\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0433 - accuracy: 0.9986\n",
            "Epoch 17/25\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0379 - accuracy: 0.9986\n",
            "Epoch 18/25\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0334 - accuracy: 0.9986\n",
            "Epoch 19/25\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0297 - accuracy: 0.9986\n",
            "Epoch 20/25\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.9986\n",
            "Epoch 21/25\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0237 - accuracy: 0.9986\n",
            "Epoch 22/25\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.9986\n",
            "Epoch 23/25\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0193 - accuracy: 0.9986\n",
            "Epoch 24/25\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0175 - accuracy: 0.9986\n",
            "Epoch 25/25\n",
            "70/70 [==============================] - 0s 2ms/step - loss: 0.0160 - accuracy: 0.9986\n",
            "22/22 [==============================] - 0s 2ms/step - loss: 0.0149 - accuracy: 0.9986\n"
          ]
        }
      ],
      "source": [
        "#Train it\n",
        "model.fit(dt_train_vect, y_train, epochs=25, batch_size=10)\n",
        "# Evaluate it\n",
        "_, accuracy = model.evaluate(dt_train_vect, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bee22554",
      "metadata": {
        "id": "bee22554"
      },
      "source": [
        "#### The accuracy for Neural and Logistic regression\n",
        "\n",
        "Neural Network = 0.9986\n",
        "\n",
        "Logistric regression = 0.79"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a0de27e",
      "metadata": {
        "id": "4a0de27e"
      },
      "outputs": [],
      "source": [
        "#Generate classification model for the Yelp dataset  with   K-NN algorithms\n",
        "#test the model for different values for K (from 1 to 5) using a for loop and record and plot the KNNs testing accuracy in a variable (scores).  \n",
        "yp_yelp = pd.read_csv('yelp_data.txt', sep='\\t', names=['sentence', 'label'], encoding = 'unicode_escape')\n",
        "yp_yelp = pd.DataFrame(yp_yelp)\n",
        "yp_yelp['source'] = 'yelp'\n",
        "yp_yelp.values\n",
        "yp_yelp\n",
        "dt = yp_yelp['sentence'].values\n",
        "y1 = yp_yelp['label'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cd679f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cd679f6",
        "outputId": "b48d5224-5125-48ce-9803-3abe519fc5f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 60)\t1\n",
            "  (0, 353)\t1\n",
            "  (0, 466)\t1\n",
            "  (0, 617)\t1\n",
            "  (0, 988)\t1\n",
            "  (0, 1337)\t1\n",
            "  (0, 1376)\t1\n",
            "  (0, 1438)\t1\n",
            "  (0, 1450)\t1\n",
            "  (0, 1479)\t1\n",
            "  (0, 1514)\t1\n",
            "  (0, 1537)\t1\n",
            "  (1, 75)\t1\n",
            "  (1, 151)\t1\n",
            "  (1, 442)\t1\n",
            "  (1, 466)\t1\n",
            "  (1, 686)\t1\n",
            "  (1, 764)\t1\n",
            "  (1, 783)\t1\n",
            "  (1, 915)\t1\n",
            "  (1, 988)\t1\n",
            "  (1, 1001)\t1\n",
            "  (1, 1076)\t1\n",
            "  (1, 1086)\t1\n",
            "  (1, 1087)\t1\n",
            "  :\t:\n",
            "  (696, 1587)\t1\n",
            "  (697, 41)\t1\n",
            "  (697, 640)\t1\n",
            "  (697, 1164)\t2\n",
            "  (697, 1205)\t1\n",
            "  (697, 1450)\t1\n",
            "  (697, 1476)\t1\n",
            "  (698, 586)\t1\n",
            "  (698, 640)\t1\n",
            "  (698, 935)\t1\n",
            "  (698, 979)\t1\n",
            "  (698, 1450)\t1\n",
            "  (699, 13)\t1\n",
            "  (699, 14)\t1\n",
            "  (699, 56)\t1\n",
            "  (699, 106)\t1\n",
            "  (699, 340)\t1\n",
            "  (699, 541)\t1\n",
            "  (699, 661)\t1\n",
            "  (699, 742)\t1\n",
            "  (699, 785)\t1\n",
            "  (699, 988)\t1\n",
            "  (699, 1080)\t1\n",
            "  (699, 1449)\t1\n",
            "  (699, 1587)\t2\n",
            "  (0, 112)\t1\n",
            "  (0, 633)\t1\n",
            "  (0, 974)\t1\n",
            "  (0, 1627)\t1\n",
            "  (1, 74)\t1\n",
            "  (1, 432)\t1\n",
            "  (1, 633)\t1\n",
            "  (1, 753)\t1\n",
            "  (1, 1091)\t1\n",
            "  (1, 1456)\t1\n",
            "  (1, 1467)\t1\n",
            "  (1, 1660)\t1\n",
            "  (2, 41)\t1\n",
            "  (2, 56)\t2\n",
            "  (2, 88)\t1\n",
            "  (2, 336)\t1\n",
            "  (2, 553)\t1\n",
            "  (2, 783)\t1\n",
            "  (2, 852)\t1\n",
            "  (2, 963)\t1\n",
            "  (2, 978)\t1\n",
            "  (2, 988)\t1\n",
            "  (2, 1064)\t1\n",
            "  (2, 1234)\t1\n",
            "  (2, 1422)\t1\n",
            "  :\t:\n",
            "  (296, 783)\t1\n",
            "  (296, 1450)\t1\n",
            "  (297, 46)\t1\n",
            "  (297, 56)\t1\n",
            "  (297, 88)\t1\n",
            "  (297, 640)\t1\n",
            "  (297, 816)\t1\n",
            "  (297, 1114)\t1\n",
            "  (297, 1164)\t1\n",
            "  (297, 1449)\t1\n",
            "  (297, 1450)\t1\n",
            "  (297, 1587)\t1\n",
            "  (298, 56)\t1\n",
            "  (298, 90)\t1\n",
            "  (298, 535)\t1\n",
            "  (298, 783)\t2\n",
            "  (298, 970)\t1\n",
            "  (298, 1065)\t1\n",
            "  (298, 1260)\t1\n",
            "  (298, 1275)\t1\n",
            "  (298, 1450)\t1\n",
            "  (298, 1479)\t1\n",
            "  (299, 104)\t1\n",
            "  (299, 1086)\t1\n",
            "  (299, 1114)\t1\n"
          ]
        }
      ],
      "source": [
        "xdt_train, xdt_test, y_train1, y_test1 = train_test_split(xdata, y1, test_size=0.30, random_state=1000)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(xdata_train)\n",
        "xdt_train_vect=vectorizer.transform(xdt_train)\n",
        "xdt_test_vect=vectorizer.transform(xdt_test)\n",
        "print(xdata_train_vect)\n",
        "print(xdata_test_vect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74e07a2c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74e07a2c",
        "outputId": "825461d0-76d3-4b16-ce55-fbe77574b04a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6366666666666667\n",
            "Accuracy: 0.5966666666666667\n",
            "Accuracy: 0.62\n",
            "Accuracy: 0.6266666666666667\n",
            "Accuracy: 0.6533333333333333\n",
            "Accuracy: 0.6533333333333333\n",
            "Accuracy: 0.6066666666666667\n",
            "Accuracy: 0.5966666666666667\n",
            "Accuracy: 0.6166666666666667\n",
            "Accuracy: 0.62\n",
            "Accuracy: 0.64\n",
            "Accuracy: 0.6166666666666667\n",
            "Accuracy: 0.6433333333333333\n",
            "Accuracy: 0.6433333333333333\n",
            "Accuracy: 0.6633333333333333\n",
            "Accuracy: 0.65\n",
            "Accuracy: 0.67\n",
            "Accuracy: 0.6666666666666666\n",
            "Accuracy: 0.65\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "k_range = range(1,20)\n",
        "scores = {}\n",
        "scores_list = []\n",
        "for k in k_range:\n",
        "    knn=KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(xdt_train_vect, y_train1)\n",
        "    knnscore = knn.score(xdt_test_vect, y_test1)\n",
        "    print(\"Accuracy:\", knnscore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f26867a9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "f26867a9",
        "outputId": "5194b271-932b-4c41-94b9-99073be9caa8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# import Matplotlib (scientific plotting library)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# allow plots to appear within the notebook\n",
        "%matplotlib inline\n",
        "\n",
        "#Plot the relationship between k and testing accuracy\n",
        "plt.plot(k_range)\n",
        "plt.xlabel('Value of K for KNN')\n",
        "plt.ylabel('Testing Accuracy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3abae67c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3abae67c",
        "outputId": "5f808bff-6b23-464c-b539-01150aa73184"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              sentence  label  source\n",
            "0    So there is no way for me to plug it in here i...      0  amazon\n",
            "1                          Good case, Excellent value.      1  amazon\n",
            "2                               Great for the jawbone.      1  amazon\n",
            "3    Tied to charger for conversations lasting more...      0  amazon\n",
            "4                                    The mic is great.      1  amazon\n",
            "..                                                 ...    ...     ...\n",
            "995  The screen does get smudged easily because it ...      0  amazon\n",
            "996  What a piece of junk.. I lose more calls on th...      0  amazon\n",
            "997                       Item Does Not Match Picture.      0  amazon\n",
            "998  The only thing that disappoint me is the infra...      0  amazon\n",
            "999  You can not answer calls with the unit, never ...      0  amazon\n",
            "\n",
            "[1000 rows x 3 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None]"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ],
      "source": [
        "##Generate prediction for the following  reviews  based Logistic regression , classifier  in Amazon dataset:\n",
        "#Review1 = \"SUPERB, I AM IN LOVE IN THIS PHONE\"   \n",
        "#Review 2 = \"Do not purchase this product. My cell phone blast when  I switched the charger\"   \n",
        "mz_amazon = pd.read_csv('amazon_data.txt', sep='\\t', names=['sentence', 'label'], encoding = 'unicode_escape')\n",
        "mz_amazon = pd.DataFrame(mz_amazon)\n",
        "mz_amazon['source'] = 'amazon'\n",
        "mz_amazon.values\n",
        "mz_amazon\n",
        "ax = mz_amazon['sentence'].values\n",
        "ay = mz_amazon['label'].values\n",
        "[print(mz_amazon)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4095c2c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4095c2c7",
        "outputId": "10c7a888-ef0c-4a60-d054-e9ab000e8bb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500,)\n",
            "(500,)\n",
            "(500,)\n",
            "(500,)\n"
          ]
        }
      ],
      "source": [
        "mX_train = mz_amazon.loc[0:499, 'sentence'].values\n",
        "mY_train = mz_amazon.loc[0:499, 'label'].values\n",
        "mX_test = mz_amazon.loc[500:999, 'sentence'].values\n",
        "mY_test = mz_amazon.loc[500:999, 'label'].values\n",
        "print(mX_train.shape)\n",
        "print(mY_train.shape)\n",
        "print(mX_test.shape)\n",
        "print(mY_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d479ede0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d479ede0",
        "outputId": "7627e0a1-1893-45bf-bb3b-44a861ce39b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500, 1139)\n",
            "(500, 1139)\n"
          ]
        }
      ],
      "source": [
        "count_vect = CountVectorizer()\n",
        "mc_train = count_vect.fit_transform(mX_train)\n",
        "mc_test = count_vect.transform(mX_test)\n",
        "print(mc_train.shape)\n",
        "print(mc_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9323e8e",
      "metadata": {
        "id": "c9323e8e"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Create feature vectors\n",
        "vectorizer = TfidfVectorizer(min_df = 5,\n",
        "                             max_df = 0.8,\n",
        "                             sublinear_tf = True,\n",
        "                             use_idf = True)\n",
        "train_vectors = vectorizer.fit_transform(mX_train)\n",
        "test_vectors = vectorizer.transform(mX_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38857b3b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38857b3b",
        "outputId": "2768d93c-b74c-4ef6-9b72-6fb38c62cd8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time: 0.019202s; Prediction time: 0.000389s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report\n",
        "Logis = LogisticRegression()\n",
        "t0 = time.time()\n",
        "Logis.fit(train_vectors, mY_train)\n",
        "t1 = time.time()\n",
        "prediction_logis = Logis.predict(test_vectors)\n",
        "t2 = time.time()\n",
        "time_logis_train = t1-t0\n",
        "time_logis_predict = t2-t1\n",
        "print(\"Training time: %fs; Prediction time: %fs\" % (time_logis_train, time_logis_predict))\n",
        "report = classification_report(tY_test, prediction_logis)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "945d7c04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "945d7c04",
        "outputId": "64d0b3b1-01a2-4940-c822-283346f171d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n"
          ]
        }
      ],
      "source": [
        "review = \"\"\"SUPERB, I AM IN LOVE IN THIS PHONE\"\"\"\n",
        "review_vector = vectorizer.transform([review])\n",
        "print(Logis.predict(review_vector))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3gf6Ehi6crBI"
      },
      "id": "3gf6Ehi6crBI",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "bee22554"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}